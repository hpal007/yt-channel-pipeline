# Youtube channel data pipeline

A data pipeline for extracting and processing YouTube channel data, including video details, and comments, built using Apache Airflow.

## Overview

This project automates the process of extracting data from YouTube channels using the YouTube Data API and storing it in a PostgreSQL database. It uses Apache Airflow to define, schedule, and monitor the data pipeline.

## Project Structure

The repository is organized as follows:

-   `.env`: Stores environment variables, such as the YouTube API key.
-   `.gitignore`: Specifies intentionally untracked files that Git should ignore.
-   `docker-compose.yaml`: Defines the services, networks, and volumes for running the Airflow environment using Docker.
-   `README.md`: Provides an overview of the project and instructions for setup and usage.
-   `config/`: Contains the Airflow configuration file (`airflow.cfg`).
-   `dags/`: Contains the DAG (Directed Acyclic Graph) definition files for Airflow:
    -   `api_data_processing.py`: Defines a DAG for processing data extracted via API.
    -   `youtube_data_pipeline.py`: Defines the main YouTube data pipeline DAG.
-   `logs/`: Stores the logs generated by Airflow tasks.
-   `plugins/`: Contains custom Airflow plugins (currently empty).
-   `src/`: Contains source code for the data extraction and processing scripts.

## Key Components

-   **Apache Airflow:** A platform for programmatically authoring, scheduling, and monitoring workflows.
-   **YouTube Data API:** Used to extract channel, video, and comment data from YouTube.
-   **PostgreSQL:** A relational database used to store the extracted data.
-   **Docker Compose:** A tool for defining and running multi-container Docker applications.

## DAGs

The project includes the following Airflow DAGs:

-   `youtube_data_pipeline`: This DAG extracts YouTube channel data, playlist items, video details, and comments.
-   `api_data_processing`: This DAG processes data extracted via the YouTube API and stores comments in a PostgreSQL database.

## Configuration

-   Airflow is configured using `config/airflow.cfg`.
-   Environment variables, including the YouTube API key, are stored in `.env`.

## Getting Started

1.  **Set up the environment:**
    -   Install Docker and Docker Compose.
    -   Obtain a YouTube Data API key and store it in the `.env` file.
2.  **Run the Airflow environment:**

    ```bash
    docker-compose up -d
    ```

3.  **Access the Airflow UI:**
    -   Open a web browser and navigate to `http://localhost:8080`.
    -   Log in with the default credentials (username: `airflow`, password: `airflow`).
4.  **Trigger the DAGs:**
    -   In the Airflow UI, locate the `youtube_data_pipeline` and `api_data_processing` DAGs.
    -   Unpause the DAGs and trigger them to start the data extraction and processing pipeline.

# Reference Links
[YouTube Data API](https://developers.google.com/youtube/v3/docs)

[API Creation and Quota & System Limit](https://console.cloud.google.com/apis/api/youtube.googleapis.com/)